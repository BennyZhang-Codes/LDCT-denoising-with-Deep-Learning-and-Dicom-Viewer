{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 两期神经网络训练"
   ]
  },
  {
   "source": [
    "import math\n",
    "import os\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm import trange\n",
    "\n",
    "import dataset\n",
    "from Nets import TS_Net, TS_DenseNet \n",
    "from Loss_Func import SSIM_Loss, MSE_Loss\n",
    "from Model_Test import ModelTest"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1、载入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LDCT_path = r'E:\\NBIA\\Sampling\\Dataset\\LDCT5set'\n",
    "NDCT_path = r'E:\\NBIA\\Sampling\\Dataset\\NDCT5set'\n",
    "\n",
    "my_totensor = dataset.My_ToTensor()\n",
    "my_normalize = dataset.My_Normalize(0.1225, 0.1188)\n",
    "transform = dataset.My_Compose([my_totensor])\n",
    "normalize = dataset.My_Compose([my_normalize])\n",
    "\n",
    "train_set = dataset.Mydataset(LDCT_root = LDCT_path, NDCT_root = NDCT_path, train = True, \n",
    "                      transform = transform, \n",
    "                      normalize = normalize)\n",
    "train_loader = torch.utils.data.DataLoader(train_set,\n",
    "                                           batch_size = 16, \n",
    "                                           num_workers = 4,\n",
    "                                           shuffle = True,)\n",
    "print(train_set.len)"
   ]
  },
  {
   "source": [
    "## 2、构建神经网络"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = TS_Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3、定义损失函数和优化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion_MSE = nn.MSELoss(reduction = 'mean') #定义损失函数：均方误差\n",
    "criterion_SSIM = SSIM_Loss()\n",
    "optimizer_MSE = optim.Adam(net.S.parameters(), lr = 0.001) #定义优化方法，Adam\n",
    "optimizer_SSIM = optim.Adam(net.F.parameters(), lr = 0.001) #定义优化方法，Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4、模型训练\n",
    "### a.参数加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------训练设置------#\n",
    "model_name = 'TS_Net'\n",
    "train_epoch_num = 5\n",
    "First_train = False\n",
    "train_adjust = False\n",
    "\n",
    "#----------------------------分割线----------------------------#\n",
    "loss_path_MSE = r'./{0}/{0}_Loss_MSE.npy'.format(model_name)\n",
    "loss_path_SSIM = r'./{0}/{0}_Loss_SSIM.npy'.format(model_name)\n",
    "epochs_loss_path_MSE = r'./{0}/{0}_Epochs_Loss_MSE.npy'.format(model_name)\n",
    "epochs_loss_path_SSIM = r'./{0}/{0}_Epochs_Loss_SSIM.npy'.format(model_name)\n",
    "if not os.path.isdir('./{}'.format(model_name)):\n",
    "    os.mkdir('./{}'.format(model_name))\n",
    "if First_train:\n",
    "    epoch_now = 0\n",
    "    all_epoch_loss_MSE = []\n",
    "    epochs_loss_MSE = []\n",
    "    all_epoch_loss_SSIM = []\n",
    "    epochs_loss_SSIM = []\n",
    "    \n",
    "else:\n",
    "    model_num = int(input('The num of the last Epoch: '))\n",
    "    model_path = r'./{0}/checkpoint/{0}_epoch_{1}.ckpt'.format(model_name, model_num)\n",
    "    checkpoint = torch.load(model_path)\n",
    "    net.F.load_state_dict(checkpoint['net.F'])\n",
    "    net.S.load_state_dict(checkpoint['net.S'])\n",
    "    optimizer_MSE.load_state_dict(checkpoint['optimizer_MSE'])\n",
    "    for state in optimizer_MSE.state.values():\n",
    "        for k, v in state.items():\n",
    "            if torch.is_tensor(v):\n",
    "                state[k] = v.cuda()\n",
    "    optimizer_SSIM.load_state_dict(checkpoint['optimizer_SSIM'])\n",
    "    for state in optimizer_SSIM.state.values():\n",
    "        for k, v in state.items():\n",
    "            if torch.is_tensor(v):\n",
    "                state[k] = v.cuda()\n",
    "    epoch_now = checkpoint['epoch']\n",
    "\n",
    "    all_epoch_loss_MSE = np.load(loss_path_MSE)\n",
    "    all_epoch_loss_MSE = list(all_epoch_loss_MSE)\n",
    "    epochs_loss_MSE = np.load(epochs_loss_path_MSE)\n",
    "    epochs_loss_MSE = list(epochs_loss_MSE)\n",
    "    all_epoch_loss_SSIM = np.load(loss_path_SSIM)\n",
    "    all_epoch_loss_SSIM = list(all_epoch_loss_SSIM)\n",
    "    epochs_loss_SSIM = np.load(epochs_loss_path_SSIM)\n",
    "    epochs_loss_SSIM = list(epochs_loss_SSIM)\n",
    "    if train_adjust:\n",
    "        file_path = r'./{0}/{0}-{1}.json'.format(model_name,model_num)\n",
    "        with open(file_path, 'r') as f:\n",
    "            d = json.load(f)\n",
    "        all_epoch_loss_MSE  = d['all_epoch_loss_MSE']\n",
    "        all_epoch_loss_SSIM = d['all_epoch_loss_SSIM']\n",
    "        epochs_loss_SSIM = d['epochs_loss_SSIM']\n",
    "        epochs_loss_MSE  = d['epochs_loss_MSE']\n",
    "    print('Epoch:', epoch_now)\n",
    "    print(len(all_epoch_loss_MSE))\n",
    "    \n",
    "scheduler_MSE = optim.lr_scheduler.ExponentialLR(optimizer_MSE, gamma = 0.75, last_epoch = epoch_now-1, verbose=True)\n",
    "scheduler_SSIM = optim.lr_scheduler.ExponentialLR(optimizer_SSIM, gamma = 0.75, last_epoch = epoch_now-1, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b.训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "net.cuda()\n",
    "net.train()\n",
    "\n",
    "for epoch in range(train_epoch_num):\n",
    "    dataiter = iter(train_loader)\n",
    "    loss_list_MSE = []\n",
    "    loss_list_SSIM = []\n",
    "    \n",
    "    for batch_idx in trange(len(train_loader)):\n",
    "        #初始化\n",
    "        data = dataiter.next()\n",
    "        LDCT_img, NDCT_img = data\n",
    "        LDCT_img = LDCT_img.cuda()\n",
    "        NDCT_img = NDCT_img.cuda()\n",
    "\n",
    "        # 将梯度设置为0\n",
    "        optimizer_MSE.zero_grad()\n",
    "        optimizer_SSIM.zero_grad()             \n",
    "\n",
    "        F_x, S_x = net(LDCT_img)\n",
    "        F_x = F_x.cpu()\n",
    "        \n",
    "        #预测结果predicted_res和residuals通过之前定义的MSE计算损失\n",
    "        MSE_loss = criterion_MSE(S_x, NDCT_img)\n",
    "        NDCT_img = NDCT_img.cpu()\n",
    "        SSIM_loss = criterion_SSIM(F_x, NDCT_img)\n",
    "\n",
    "        # 误差反向传播MSE\n",
    "        MSE_loss.backward()\n",
    "        SSIM_loss.backward()\n",
    "\n",
    "        # Adam优化权重\n",
    "        optimizer_MSE.step()  \n",
    "        # optimizer_SSIM.step()                 \n",
    "\n",
    "        # 保存Loss\n",
    "        loss_list_MSE.append(MSE_loss.item())\n",
    "        loss_list_SSIM.append(SSIM_loss.item())\n",
    "\n",
    "    mean_loss_MSE = sum(loss_list_MSE)/len(loss_list_MSE)\n",
    "    mean_loss_SSIM = sum(loss_list_SSIM)/len(loss_list_SSIM)\n",
    "    #保存Loss数据\n",
    "    epochs_loss_MSE.append(mean_loss_MSE)\n",
    "    epochs_loss_SSIM.append(mean_loss_SSIM)\n",
    "\n",
    "    save_epochs_loss_MSE = np.array(epochs_loss_MSE)\n",
    "    save_epochs_loss_SSIM = np.array(epochs_loss_SSIM)\n",
    "\n",
    "    np.save(epochs_loss_path_MSE, save_epochs_loss_MSE)\n",
    "    np.save(epochs_loss_path_SSIM, save_epochs_loss_SSIM)\n",
    "\n",
    "    all_epoch_loss_MSE += loss_list_MSE\n",
    "    all_epoch_loss_SSIM += loss_list_SSIM\n",
    "\n",
    "    save_loss_MSE = np.array(all_epoch_loss_MSE)\n",
    "    save_loss_SSIM = np.array(all_epoch_loss_SSIM)\n",
    "    \n",
    "    np.save(loss_path_MSE, save_loss_MSE)\n",
    "    np.save(loss_path_SSIM, save_loss_SSIM)\n",
    "\n",
    "    print('[epoch %d]: %.10f %.10f' % (epoch+epoch_now+1, mean_loss_MSE, mean_loss_SSIM))\n",
    "    print('[epoch %d]: %.10f %.10f' % (epoch+epoch_now+1, mean_loss_MSE, mean_loss_MSE))\n",
    "    plt.figure(figsize=(15,3))\n",
    "    plt.subplot(231), plt.title('Epoch{}'.format(epoch+epoch_now+1)), plt.plot(loss_list_MSE, color='brown')\n",
    "    plt.subplot(232), plt.title('All_Loss_MSE'), plt.plot(all_epoch_loss_MSE, color='brown')\n",
    "    plt.subplot(233), plt.title('Epochs_Loss_MSE'), plt.plot(epochs_loss_MSE, color='brown')\n",
    "    plt.subplot(234), plt.title('Epoch{}'.format(epoch+epoch_now+1)), plt.plot(loss_list_SSIM, color='teal')\n",
    "    plt.subplot(235), plt.title('All_Loss_SSIM'), plt.plot(all_epoch_loss_SSIM, color='teal')\n",
    "    plt.subplot(236), plt.title('Epochs_Loss_SSIM'), plt.plot(epochs_loss_SSIM, color='teal')\n",
    "    plt.tight_layout(pad=0, w_pad=1.5, h_pad=0.5)\n",
    "    plt.show()\n",
    "\n",
    "    print('Saving epoch %d model ...' % (epoch+epoch_now+1))\n",
    "\n",
    "    state = {'net.F': net.F.state_dict(), 'net.S': net.S.state_dict(), 'optimizer_MSE': optimizer_MSE.state_dict(), 'optimizer_SSIM': optimizer_SSIM.state_dict(),'epoch': epoch+epoch_now+1}\n",
    "    if not os.path.isdir('./{}/checkpoint'.format(model_name)):\n",
    "        os.mkdir('./{}/checkpoint'.format(model_name))\n",
    "    torch.save(state, './{0}/checkpoint/{0}_epoch_{1}.ckpt'.format(model_name, epoch+epoch_now+1))\n",
    "    #调整Lr\n",
    "    scheduler_MSE.step()\n",
    "    scheduler_SSIM.step()\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.subplot(221), plt.title('All_Loss_MSE'), plt.plot(all_epoch_loss_MSE, color='brown')\n",
    "plt.subplot(222), plt.title('Epochs_Loss_MSE'), plt.plot(epochs_loss_MSE, color='brown')\n",
    "plt.subplot(223), plt.title('All_Loss_SSIM'), plt.plot(all_epoch_loss_SSIM, color='teal')\n",
    "plt.subplot(224), plt.title('Epochs_Loss_SSIM'), plt.plot(epochs_loss_SSIM, color='teal')\n",
    "plt.tight_layout(pad=1, w_pad=1.5, h_pad=0.5)\n",
    "plt.savefig('./{}/LOSS-Epoch-{}.jpg'.format(model_name, epoch+epoch_now+1), dpi = 500, bbox_inches = 'tight', pad_inches = 0.25)\n",
    "plt.show()\n",
    "\n",
    "torch.save(net, './{0}/checkpoint/{0}_epoch_{1}.pt'.format(model_name, epoch+epoch_now+1))\n",
    "print('Finished Training...')\n",
    "for idx in range(len(epochs_loss_MSE)):\n",
    "    print('Epoch{:3}: '.format(idx+1), epochs_loss_MSE[idx], epochs_loss_MSE[idx])###"
   ]
  },
  {
   "source": [
    "### c.保存训练数据"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "res_dict = {}\n",
    "res_dict['model_name'] = model_name\n",
    "res_dict['loss_decay'] = 0.75\n",
    "res_dict['batch_size'] = 16\n",
    "res_dict['model_config'] = {'self.F':'self.S = DenseNet(growth_rate=16, block_config=(4, 8, 4), num_init_features=64, bn_size=4)',\n",
    "'self.S':'self.S = DenseNet(growth_rate=16, block_config=(4, 8, 4), num_init_features=64, bn_size=4)'}\n",
    "res_dict['net'] = str(net)\n",
    "res_dict['epochs'] = '{}'.format(epoch+epoch_now+1)\n",
    "res_dict['epochs_loss_MSE'] = epochs_loss_MSE\n",
    "res_dict['epochs_loss_SSIM'] = epochs_loss_SSIM\n",
    "res_dict['all_epoch_loss_MSE'] = all_epoch_loss_MSE\n",
    "res_dict['all_epoch_loss_SSIM'] = all_epoch_loss_SSIM\n",
    "file_path = r'./{0}/{0}-{1}.json'.format(model_name,epoch+epoch_now+1)\n",
    "with open(file_path, 'w') as f:\n",
    "    json.dump(res_dict, f, ensure_ascii = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5、模型测试\n",
    "### a.测试集测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LDCT_path = r'E:\\NBIA\\LDCT-and-Projection-data\\L123\\08-23-2018-75696\\1.000000-Low Dose Images-07574'\n",
    "NDCT_path = r'E:\\NBIA\\LDCT-and-Projection-data\\L123\\08-23-2018-75696\\1.000000-Full dose images-67226'\n",
    "test_epoch = 30\n",
    "model_path = r'./{0}/checkpoint/{0}_epoch_{1}.pt'.format(model_name, test_epoch)\n",
    "save_path = r'./{}/Test_Result'.format(model_name)\n",
    "test = ModelTest(model_name, test_epoch, model_path, LDCT_path, NDCT_path, save_path, stage_num=2)\n",
    "test.run()"
   ]
  },
  {
   "source": [
    "### b.单图测试"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_epoch = 30\n",
    "ND_path = r'E:\\NBIA\\L004\\LDCT-and-Projection-data\\L004\\08-21-2018-84608\\1.000000-Full dose images\\1-44.dcm'\n",
    "LD_path = r'E:\\NBIA\\L004\\LDCT-and-Projection-data\\L004\\08-21-2018-84608\\1.000000-Low Dose Images\\1-44.dcm'\n",
    "net_test = torch.load('./{0}/checkpoint/{0}_epoch_{1}.pt'.format(model_name, test_epoch))\n",
    "\n",
    "net_test.eval()\n",
    "net_test.cuda()\n",
    "\n",
    "import pydicom.filereader as dcmreader\n",
    "import pydicom.dataset as dcmdt\n",
    "\n",
    "def Window(WW, WL):\n",
    "    win_dict = {'vmin':WL-WW/2, 'vmax':WL+WW/2}\n",
    "    return win_dict\n",
    "def array2tensor(image_array):\n",
    "    image_array = image_array[:, :, None]\n",
    "    image_array = torch.from_numpy(image_array.transpose((2, 0, 1))).contiguous()\n",
    "    image_array = torch.stack([image_array])\n",
    "    image_tensor = image_array/1.0\n",
    "    return image_tensor\n",
    "\n",
    "LD_ds = dcmreader.dcmread(LD_path)\n",
    "ND_ds = dcmreader.dcmread(ND_path)\n",
    "LD_img = LD_ds.pixel_array.astype(np.int16)\n",
    "ND_img = ND_ds.pixel_array.astype(np.int16)\n",
    "my_totensor = dataset.My_ToTensor()\n",
    "my_normalize = dataset.My_Normalize(0.1225, 0.1188)\n",
    "loader = dataset.My_Compose([my_totensor, my_normalize])\n",
    "Res_img = LD_img - ND_img\n",
    "print(ND_img.shape)\n",
    "plt.subplot(131), plt.title('ND'), plt.imshow(ND_img-1024, cmap = plt.cm.Greys_r, **Window(300, 40)), plt.axis('off')\n",
    "plt.subplot(132), plt.title('LD'), plt.imshow(LD_img-1024, cmap = plt.cm.Greys_r, **Window(300, 40)), plt.axis('off')\n",
    "plt.subplot(133), plt.title('Res'),  plt.imshow(Res_img, cmap = plt.cm.Greys_r), plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "LD_img = loader(LD_img)\n",
    "ND_img = loader(ND_img)\n",
    "LD_img = torch.stack([LD_img])\n",
    "ND_img = ND_img.squeeze()\n",
    "ND_img = ND_img.detach().numpy()\n",
    "\n",
    "with torch.no_grad():\n",
    "    LD_img = LD_img.cuda()\n",
    "    F, S = net_test(LD_img)\n",
    "    pre = S.cpu()\n",
    "    LD_img = LD_img.cpu()\n",
    "    LD_img = LD_img.squeeze()\n",
    "    LD_img = LD_img.detach().numpy()\n",
    "    pre = pre.squeeze()\n",
    "    pre = pre.detach().numpy()\n",
    "\n",
    "plt.subplot(231), plt.axis('off'), plt.title('NDCT'),             plt.imshow(((ND_img*0.1188)+0.1225)*4096 - 1024,\n",
    "                                                                             cmap = plt.cm.Greys_r, **Window(300, 40))\n",
    "plt.subplot(232), plt.axis('off'), plt.title('LDCT'),             plt.imshow(((LD_img*0.1188)+0.1225)*4096 - 1024,\n",
    "                                                                             cmap = plt.cm.Greys_r, **Window(300, 40))\n",
    "plt.subplot(233), plt.axis('off'), plt.title('Predicted'),        plt.imshow(((pre*0.1188)+0.1225)*4096 - 1024,\n",
    "                                                                             cmap = plt.cm.Greys_r, **Window(300, 40))\n",
    "plt.subplot(234), plt.axis('off'), plt.title('LDCT - NDCT'),      plt.imshow(LD_img - ND_img, cmap = plt.cm.Greys_r)\n",
    "plt.subplot(235), plt.axis('off'), plt.title('LDCT - Predicted'), plt.imshow(LD_img - pre, cmap = plt.cm.Greys_r)\n",
    "plt.subplot(236), plt.axis('off'), plt.title('Predicted - NDCT'), plt.imshow(pre - ND_img,\n",
    "                                                                             cmap = plt.cm.Greys_r)\n",
    "(0.1225, 0.1188)\n",
    "plt.imsave('./{}/{}-1 ND.jpg'.format(model_name, test_epoch), ND_img, cmap = plt.cm.Greys_r)\n",
    "plt.imsave('./{}/{}-2 LD.jpg'.format(model_name, test_epoch), LD_img, cmap = plt.cm.Greys_r)\n",
    "plt.imsave('./{}/{}-3 pre.jpg'.format(model_name, test_epoch), pre, cmap = plt.cm.Greys_r)\n",
    "plt.imsave('./{}/{}-4 pre_Noise.jpg'.format(model_name, test_epoch), LD_img - pre, cmap = plt.cm.Greys_r)\n",
    "plt.imsave('./{}/{}-5 rea_Noise.jpg'.format(model_name, test_epoch), LD_img - ND_img, cmap = plt.cm.Greys_r)\n",
    "plt.savefig('./{}/Epoch-{}.jpg'.format(model_name, test_epoch), dpi = 500, bbox_inches = 'tight', pad_inches = 0.25)\n",
    "\n",
    "def mseloss(x, y):\n",
    "    return np.sum((x-y)**2)/(512*512)\n",
    "\n",
    "LDloss = mseloss(LD_img, ND_img)\n",
    "preloss = mseloss(pre, ND_img)\n",
    "print('LD:  ', LDloss,  10*np.log10(2*2/LDloss))\n",
    "print('pre: ', preloss, 10*np.log10(2*2/preloss))\n",
    "\n",
    "pn = (((pre*0.1188)+0.1225)*4096)\n",
    "pn = np.clip(pn, 0, 4096)\n",
    "pn = pn.astype(np.uint16)\n",
    "\n",
    "ND_ds.SeriesDescription = 'NDCT'\n",
    "LD_ds.SeriesDescription = 'LDCT'\n",
    "ND_ds.save_as(\"./{}/NDCT.dcm\".format(model_name))\n",
    "LD_ds.save_as(\"./{}/LDCT.dcm\".format(model_name))\n",
    "\n",
    "ND_ds.SeriesDescription = 'Predicted'\n",
    "ND_ds.PixelData = pn.tobytes()\n",
    "ND_ds.Rows, ND_ds.Columns = pn.shape\n",
    "ND_ds.save_as(\"./{}/Predicted.dcm\".format(model_name))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python385jvsc74a57bd0f7a73bb8ca7f678f26aafc5e0ccf3837630645dd5ed8595cc84348c81285d859",
   "display_name": "Python 3.8.5 64-bit ('LDCT': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}